{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "149b2ecc",
   "metadata": {},
   "source": [
    "# Proyecto 1: Regresión Lineal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccaecc9",
   "metadata": {},
   "source": [
    "# Parte 1: Preparación y Exploracion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dc6397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# carga de libererías:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc816c2",
   "metadata": {},
   "source": [
    "### Carga del dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f490c184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el dataset\n",
    "df = pd.read_csv('dataset_proyecto_regresion.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd941ebf",
   "metadata": {},
   "source": [
    "### EDA básica y Detección de outliers: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9350b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. EDA Básica --- \n",
    "\n",
    "print(\"\\n--- EDA Básica ---\")\n",
    "\n",
    "# a. Distribución y tipos de variables\n",
    "print(\"\\nInformación general del DataFrame:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nPrimeras 5 filas del DataFrame:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nEstadísticas descriptivas de las variables numéricas:\")\n",
    "print(df.describe())\n",
    "\n",
    "# b. Valores faltantes\n",
    "print(\"\\nValores faltantes por columna:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\nPorcentaje de valores faltantes por columna:\")\n",
    "print((df.isnull().sum() / len(df)) * 100)\n",
    "\n",
    "# c. Distribución de variables categóricas\n",
    "print(\"\\nDistribución de variables categóricas:\")\n",
    "categorical_cols = df.select_dtypes(include='object').columns\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\nColumna: {col}\")\n",
    "    print(df[col].value_counts())\n",
    "    print(f\"Número de categorías únicas: {df[col].nunique()}\")\n",
    "\n",
    "\n",
    "# Visualización de distribuciones para variables numéricas clave\n",
    "numeric_cols_for_hist = ['Age', 'AnnualIncome', 'HoursPerWeek', 'CommuteDistance', 'CreditScore', 'Debt', 'Savings', 'InternetUsage', 'TargetSpend']\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(numeric_cols_for_hist):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    sns.histplot(df[col], kde=True)\n",
    "    plt.title(f'Distribución de {col}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualización para variables categóricas importantes \n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='EducationLevel', y='TargetSpend', data=df)\n",
    "plt.title('TargetSpend vs. EducationLevel')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b76caf9",
   "metadata": {},
   "source": [
    "### Manejo de valores faltantes: imputación categórica y numérica. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219ce518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Manejo de Valores Faltantes: Imputación Categórica y Numérica ---\n",
    "\n",
    "# Imputación de valores faltantes para variables categóricas (usando la moda)\n",
    "# Columnas identificadas con NaN en el EDA:\n",
    "# 'MaritalStatus': 305 NaN\n",
    "# 'Employed': 96 NaN\n",
    "# 'OwnsHouse': 74 NaN\n",
    "# 'EducationLevel': 25 NaN\n",
    "# 'Gender': 32 NaN\n",
    "categorical_cols_to_impute = ['MaritalStatus', 'Employed', 'OwnsHouse', 'EducationLevel', 'Gender']\n",
    "\n",
    "for col in categorical_cols_to_impute:\n",
    "    if df[col].isnull().any(): # Solo imputar si hay NaNs en la columna\n",
    "        mode_val = df[col].mode()[0]\n",
    "        df[col].fillna(mode_val, inplace=True)\n",
    "\n",
    "# Imputación de valores faltantes para variables numéricas (usando la mediana)\n",
    "# Columnas identificadas con NaN en el EDA:\n",
    "# 'Age': 45 NaN\n",
    "# 'AnnualIncome': 45 NaN\n",
    "# 'CreditScore': 45 NaN\n",
    "# 'Debt': 45 NaN\n",
    "# 'Savings': 45 NaN\n",
    "numeric_cols_to_impute = ['Age', 'AnnualIncome', 'CreditScore', 'Debt', 'Savings']\n",
    "\n",
    "for col in numeric_cols_to_impute:\n",
    "    if df[col].isnull().any(): # Solo imputar si hay NaNs en la columna\n",
    "        median_val = df[col].median()\n",
    "        df[col].fillna(median_val, inplace=True)\n",
    "\n",
    "# Confirmar que no hay valores faltantes después de la imputación\n",
    "print(\"Valores faltantes después de la imputación:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\nPrimeras 5 filas del DataFrame después de la imputación:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7435382",
   "metadata": {},
   "source": [
    "### Conversión de fechas: extraer características como AñosDesdeRegistro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c70f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Manejo de Fechas: Conversión y Extracción de Características ---\n",
    "\n",
    "# Convertir 'RegistrationDate' a formato datetime\n",
    "df['RegistrationDate'] = pd.to_datetime(df['RegistrationDate'], errors='coerce')\n",
    "\n",
    "# Calcular la antigüedad del registro en años\n",
    "# Usamos la fecha actual (June 6, 2025) como referencia para 'AñosDesdeRegistro'.\n",
    "# Esto asegura consistencia en el cálculo de la antigüedad.\n",
    "reference_date = pd.Timestamp('2025-06-06') # Fecha de referencia explícita\n",
    "df['YearsSinceRegistration'] = (reference_date - df['RegistrationDate']).dt.days / 365.25\n",
    "\n",
    "# Extraer otras características de la fecha\n",
    "# Por ejemplo, el mes de registro podría capturar estacionalidad en el gasto.\n",
    "df['RegistrationMonth'] = df['RegistrationDate'].dt.month\n",
    "# df['RegistrationDayOfWeek'] = df['RegistrationDate'].dt.dayofweek # 0=Lunes, 6=Domingo\n",
    "\n",
    "# Eliminar la columna 'RegistrationDate' original\n",
    "df = df.drop('RegistrationDate', axis=1)\n",
    "\n",
    "print(\"\\nPrimeras 5 filas del DataFrame después del manejo de fechas:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nTipos de datos del DataFrame después del manejo de fechas:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0cd93f",
   "metadata": {},
   "source": [
    "# Parte 2: Ingeniería de Características"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3771df0f",
   "metadata": {},
   "source": [
    "### Encoding de variables categóricas: OneHotEncoding y OrdinalEncoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74816680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columnas categóricas nominales\n",
    "categorical_nominal_cols = ['EducationLevel', 'Gender', 'MaritalStatus', 'Employed', 'JobTitle', 'OwnsHouse', 'HasPet']\n",
    "\n",
    "# Columnas categóricas ordinales\n",
    "ordinal_cols = ['FitnessLevel', 'EnvironmentalAwareness']\n",
    "\n",
    "\n",
    "print(\"--- Categorías y conteos para variables Categóricas Nominales ---\")\n",
    "for col in categorical_nominal_cols:\n",
    "    print(f\"\\nColumna: '{col}'\")\n",
    "    print(df[col].value_counts())\n",
    "    print(f\"Número de categorías únicas: {df[col].nunique()}\")\n",
    "\n",
    "print(\"\\n--- Categorías y conteos para variables Categóricas Ordinales ---\")\n",
    "for col in ordinal_cols:\n",
    "    print(f\"\\nColumna: '{col}'\")\n",
    "    print(df[col].value_counts())\n",
    "    print(f\"Número de categorías únicas: {df[col].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4238bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# --- Encoding de Variables Categóricas ---\n",
    "\n",
    "# Codificación One-Hot para variables nominales\n",
    "categorical_nominal_cols = ['EducationLevel', 'Gender', 'MaritalStatus', 'Employed', 'JobTitle', 'OwnsHouse', 'HasPet']\n",
    "df = pd.get_dummies(df, columns=categorical_nominal_cols, drop_first=True)\n",
    "\n",
    "# Codificación Ordinal para variables ordinales\n",
    "ordinal_cols = ['FitnessLevel', 'EnvironmentalAwareness']\n",
    "# Asegurar que estas columnas sean de tipo 'category' con un orden explícito si no lo están ya\n",
    "# Esto es redundante si ya se hizo, pero seguro si el estado de df no es consistente\n",
    "df['FitnessLevel'] = pd.Categorical(df['FitnessLevel'], categories=[1, 2, 3, 4, 5], ordered=True)\n",
    "df['EnvironmentalAwareness'] = pd.Categorical(df['EnvironmentalAwareness'], categories=[1, 2, 3, 4, 5], ordered=True)\n",
    "\n",
    "# El encoder usará el orden especificado: [1, 2, 3, 4, 5]\n",
    "encoder = OrdinalEncoder(categories=[[1, 2, 3, 4, 5], [1, 2, 3, 4, 5]])\n",
    "df[ordinal_cols] = encoder.fit_transform(df[ordinal_cols])\n",
    "\n",
    "\n",
    "print(\"\\nPrimeras 5 filas del DataFrame después del encoding:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nTipos de datos del DataFrame después del encoding:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358a7b24",
   "metadata": {},
   "source": [
    "### Escalado: MinMaxScaler o StandardScaler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ece5a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Escalado de Características Numéricas ---\n",
    "\n",
    "# Identificar todas las columnas numéricas para escalar, excluyendo la variable objetivo 'TargetSpend'\n",
    "numeric_cols_to_scale = df.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'TargetSpend' in numeric_cols_to_scale:\n",
    "    numeric_cols_to_scale.remove('TargetSpend')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df[numeric_cols_to_scale] = scaler.fit_transform(df[numeric_cols_to_scale])\n",
    "\n",
    "\n",
    "print(\"\\nPrimeras 5 filas del DataFrame después del escalado:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nTipos de datos del DataFrame después del escalado:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c2f5a9",
   "metadata": {},
   "source": [
    "### Discretización (binning): aplicar a Age, Debt, o CreditScore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015de059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Discretización (Binning) en 'Age' ---\n",
    "\n",
    "# Discretización de 'Age' usando cuantiles (bins de igual frecuencia)\n",
    "df['Age_Group'] = pd.qcut(df['Age'], q=5, labels=['Grupo_1', 'Grupo_2', 'Grupo_3', 'Grupo_4', 'Grupo_5'], precision=0)\n",
    "\n",
    "# La nueva columna 'Age_Group' es de tipo categórico.\n",
    "# Para su uso en modelos lineales, típicamente se aplicaría One-Hot Encoding a 'Age_Group' también.\n",
    "\n",
    "print(\"\\nPrimeras 5 filas del DataFrame después de la discretización de Age:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nConteo de categorías en 'Age_Group':\")\n",
    "print(df['Age_Group'].value_counts())\n",
    "\n",
    "print(\"\\nTipos de datos del DataFrame después de la discretización:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6979b2b1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a896b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import  PolynomialFeatures\n",
    "# --- Transformaciones Polinómicas ---\n",
    "\n",
    "# Seleccionar características para las transformaciones polinómicas.\n",
    "# Elegimos 'Age' y 'AnnualIncome' ya que son variables clave \n",
    "features_for_polynomial = ['Age', 'AnnualIncome']\n",
    "\n",
    "# Crear objeto PolynomialFeatures con grado 2 (para términos cuadrados e interacciones)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "# Transformar las características seleccionadas\n",
    "# Esto creará un array numpy con las nuevas características polinómicas\n",
    "poly_features = poly.fit_transform(df[features_for_polynomial])\n",
    "\n",
    "# Obtener los nombres de las nuevas características generadas\n",
    "poly_feature_names = poly.get_feature_names_out(features_for_polynomial)\n",
    "\n",
    "# Crear un DataFrame con las nuevas características polinómicas\n",
    "df_poly = pd.DataFrame(poly_features, columns=poly_feature_names, index=df.index)\n",
    "\n",
    "# Concatenar las nuevas características polinómicas al DataFrame original\n",
    "df = pd.concat([df, df_poly], axis=1)\n",
    "\n",
    "\n",
    "print(\"\\nPrimeras 5 filas del DataFrame después de las transformaciones polinómicas:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nTipos de datos del DataFrame después de las transformaciones polinómicas:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9472490",
   "metadata": {},
   "source": [
    "# Parte 3: Modelado "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484bc56a",
   "metadata": {},
   "source": [
    "### División de datos: entrenamiento y prueba (80/20). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac723c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- División de Datos: Entrenamiento y Prueba (80/20) ---\n",
    "\n",
    "# Separar la variable objetivo (y) de las características (X)\n",
    "X = df.drop('TargetSpend', axis=1)\n",
    "y = df['TargetSpend']\n",
    "\n",
    "# Dividir el dataset en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"\\nDimensiones de los conjuntos de datos:\")\n",
    "print(f\"X_train (entrenamiento de características): {X_train.shape}\")\n",
    "print(f\"y_train (entrenamiento de objetivo): {y_train.shape}\")\n",
    "print(f\"X_test (prueba de características): {X_test.shape}\")\n",
    "print(f\"y_test (prueba de objetivo): {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059de3ad",
   "metadata": {},
   "source": [
    "### Construcción de pipelines personalizados con BaseEstimator y TransformerMixin. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_envir_hoja_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
